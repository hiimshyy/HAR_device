{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L05pz1VkWm3e"
      },
      "outputs": [],
      "source": [
        "# cnn model with standardization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Conv1D, LSTM\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=False)\n",
        "\treturn dataframe.values"
      ],
      "metadata": {
        "id": "vjCJqhQmWuUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files into a 3D array of [samples, timesteps, features]\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded"
      ],
      "metadata": {
        "id": "dzR2SdMAW4S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y"
      ],
      "metadata": {
        "id": "gwAGodXJW5s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDataset():\n",
        "  filename = list()\n",
        "  filename += ['/content/drive/MyDrive/data_2/aX_scale.txt',\n",
        "               '/content/drive/MyDrive/data_2/aY_scale.txt',\n",
        "               '/content/drive/MyDrive/data_2/aZ_scale.txt',\n",
        "               '/content/drive/MyDrive/data_2/gX_scale.txt',\n",
        "               '/content/drive/MyDrive/data_2/gY_scale.txt',\n",
        "               '/content/drive/MyDrive/data_2/gZ_scale.txt']\n",
        "  # load input data\n",
        "  X = load_group(filename)\n",
        "  print()\n",
        "  # load class output\n",
        "  y = load_file('/content/drive/MyDrive/data_2/labels.txt')\n",
        "\n",
        "  # train test split\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, random_state = 30, test_size = 0.3)\n",
        "\n",
        "  # zero-offset class values\n",
        "  trainy = trainy-1\n",
        "  testy = testy-1\n",
        "\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "  return trainX, trainy, testX, testy #x_train, y_train, x_test, y_test, x_val, y_val"
      ],
      "metadata": {
        "id": "HeKU3OXbW7Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "KpBpkfNRxqsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX, trainy, testX, testy = loadDataset()\n",
        "\n",
        "trainX = np.where(np.isnan(trainX), np.nanmean(trainX, axis=0), trainX)\n",
        "trainy = np.where(np.isnan(trainy), np.nanmean(trainy, axis=0), trainy)\n",
        "testX = np.where(np.isnan(testX), np.nanmean(testX, axis=0), testX)\n",
        "testy = np.where(np.isnan(testy), np.nanmean(testy, axis=0), testy)\n",
        "\n",
        "print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "print(np.isnan(trainX).sum())\n",
        "print(np.isnan(trainy).sum())\n",
        "print(np.isnan(testX).sum())\n",
        "print(np.isnan(testy).sum())"
      ],
      "metadata": {
        "id": "Pj-USkD2L3RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot\n",
        "def history_plot(history):\n",
        "  pyplot.xlabel('Epoch')\n",
        "  pyplot.ylabel('value')\n",
        "  pyplot.plot(history.history['loss'], label = 'Tranning loss')\n",
        "  pyplot.plot(history.history['val_loss'], label = 'Validation loss')\n",
        "  pyplot.plot(history.history['accuracy'], label ='Traning accuracy')\n",
        "  pyplot.plot(history.history['val_accuracy'], label = 'validation accuracy')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()"
      ],
      "metadata": {
        "id": "I7Bcr0IcL8D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def CM_plot(testX, testy, model):\n",
        "  labels = [\"Sitting\", \"Standing\", \" Jumping\", \" Rotate Left\"]\n",
        "  y_pred = model.predict(testX, verbose = 1)\n",
        "  y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "  y_pred_binary = np.argmax(y_pred_binary, axis=1)\n",
        "  y_true = np.argmax(testy, axis=1)\n",
        "  cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "              xticklabels=labels, yticklabels= labels)\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"True Labels\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()\n",
        "  print(classification_report(y_true,y_pred_binary))"
      ],
      "metadata": {
        "id": "UtGljecDL8qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ROC_plot(model):\n",
        "  labels = [\"Sitting\", \"Standing\", \" Jumping\", \" Rotate Left\"]\n",
        "  y_pred = model.predict(testX, verbose = 1)\n",
        "  y_true = np.argmax(testy, axis=1)\n",
        "  y_true_one_hot = label_binarize(y_true, classes=np.unique(y_true))\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  for i in range(4):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_one_hot[:, i], y_pred[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  # Vẽ đường ROC cho từng lớp\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  for i in range(4):\n",
        "      plt.plot(fpr[i], tpr[i], label=f'Class {labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver Operating Characteristic (ROC) for each class')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-X_o6F75MAEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#epoch: 50, 100, 150\n",
        "#drop out: 0.3, 0.5, 0.7\n",
        "#optimizer: Adam, RMSProp, AdaGrad\n",
        "#kernel_size: (3), (5)\n",
        "import keras\n",
        "num_classes = len(np.unique(trainy))\n",
        "def make_model(input_shape,epochs = 50, kernel_size = 3, drop_out = 0.5, optimizer = \"adam\", batch_size = 16):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=kernel_size)(input_layer)\n",
        "    #conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "    conv1 = keras.layers.Dropout(drop_out/2)(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=32, kernel_size=kernel_size)(conv1)\n",
        "    #conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "    conv2 = keras.layers.MaxPooling1D(pool_size=4)(conv2)\n",
        "\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=32, kernel_size=kernel_size)(conv2)\n",
        "    #conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "    conv3 = keras.layers.Dropout(drop_out/2)(conv3)\n",
        "\n",
        "    conv4 = keras.layers.Conv1D(filters=32, kernel_size=kernel_size)(conv3)\n",
        "    #conv4 = keras.layers.BatchNormalization()(conv4)\n",
        "    conv4 = keras.layers.ReLU()(conv4)\n",
        "    conv4 = keras.layers.MaxPooling1D(pool_size=4)(conv4)\n",
        "\n",
        "    flatten = keras.layers.Flatten()(conv4)\n",
        "    dense = keras.layers.Dense(100, activation=\"relu\")(flatten)\n",
        "    #gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(4, activation=\"softmax\")(dense)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "    callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint( \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"),\n",
        "    keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "    history = model.fit(\n",
        "        trainX,\n",
        "        trainy,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "vZO52LpPMCB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(trainX, trainy, testX, testy):\n",
        "  repeats = 10\n",
        "  epochs = [50, 100, 150]\n",
        "  drop_out = [0.3, 0.5, 0.7]\n",
        "  optimizer= [\"Adam\", \"RMSProp\", \"AdaGrad\"]\n",
        "  kernel_size = [3, 5, 7]\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(f\"Times {r+1}\")\n",
        "    print(f\"Model with {epochs[2]} epochs, {kernel_size[0]} kernel size, {drop_out[1]} dropout, {optimizer[0]} optimizer.\")\n",
        "    model, history = make_model(input_shape=trainX.shape[1:],\n",
        "                                epochs = epochs[2],\n",
        "                                kernel_size = kernel_size[0],\n",
        "                                drop_out = drop_out[1],\n",
        "                                optimizer = optimizer[0])\n",
        "    '''keras.utils.plot_model(model,\n",
        "                           to_file='model.png',\n",
        "                           show_shapes=True,\n",
        "                           show_dtype=True,\n",
        "                           show_layer_names=True,\n",
        "                           rankdir='TB',\n",
        "                           expand_nested=True,\n",
        "                           dpi=96)'''\n",
        "    loss_test , accuracy_test = model.evaluate(testX, testy, batch_size = 1, verbose=1)\n",
        "    print(f\"Test: Accuracy = {accuracy_test}, Lost = {loss_test}\")\n",
        "    score = accuracy_test * 100.0\n",
        "    print('>Times %d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "    history_plot(history)\n",
        "    CM_plot(testX, testy, model)\n",
        "    ROC_plot(model)\n",
        "  # summarize results\n",
        "  summarize_results(scores)\n",
        "  return model, history\n",
        "model, history = run_experiments(trainX, trainy, testX, testy)"
      ],
      "metadata": {
        "id": "trpelizvMDFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "1B6PyQNoMHAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(trainX).batch(1).take(2000):\n",
        "    input_value = tf.reshape(input_value, (1, 120, 6))\n",
        "    input_value = tf.dtypes.cast(input_value, tf.float32)\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "# Save the model to disk\n",
        "open('model_quantized.tflite', \"wb\").write(tflite_model_quant)\n",
        "\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize('model_quantized.tflite')\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)\n",
        "\n",
        "tf.lite.experimental.Analyzer.analyze(model_path='model_quantized.tflite')"
      ],
      "metadata": {
        "id": "20ehS3teMI5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat model_quantized.tflite | xxd -i              >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h"
      ],
      "metadata": {
        "id": "3a5rlb7-MLeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "metadata": {
        "id": "L_iEgA_mMPCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}